import os
import json
import pandas as pd
from datetime import datetime
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

HISTORY_DIR = ".history"

class HistoryManager:
    def __init__(self, history_dir=HISTORY_DIR):
        self.history_dir = history_dir
        os.makedirs(self.history_dir, exist_ok=True)
        logging.info(f"âœ… åˆå§‹åŒ–å†å²è®°å½•ç›®å½•ï¼š{self.history_dir}")

    def get_history(self):
        metas = []
        for name in os.listdir(self.history_dir):
            folder_path = os.path.join(self.history_dir, name)
            if not os.path.isdir(folder_path):
                continue
            meta_file_path = os.path.join(folder_path, "meta.json")
            if not os.path.exists(meta_file_path):
                logging.warning(f"âš ï¸ è·³è¿‡æ— å…ƒæ•°æ®çš„ç›®å½•ï¼š{folder_path}")
                continue
            try:
                with open(meta_file_path, "r", encoding="utf-8") as f:
                    meta = json.load(f)
                metas.append(meta)
                logging.debug(f"ğŸ“¥ æˆåŠŸåŠ è½½å…ƒæ•°æ®ï¼š{meta_file_path}")
            except Exception as e:
                logging.error(f"âŒ åŠ è½½å…ƒæ•°æ®å¤±è´¥ï¼ˆ{meta_file_path}ï¼‰ï¼š{str(e)}")
                continue
        metas.sort(key=lambda x: x.get("timestamp", ""), reverse=True)
        logging.info(f"ğŸ“Š å…±åŠ è½½ {len(metas)} æ¡å†å²å¯¼å…¥è®°å½•")
        return metas

    def get_data(self, meta):
        crawl_folder = os.path.join(self.history_dir, meta["timestamp"])
        data_file_path = os.path.join(crawl_folder, "data.csv")
        anno_file_path = os.path.join(crawl_folder, "annotations.json")

        df = pd.DataFrame()
        jama = []
        gqs = []
        discern = []
        custom_data = {}

        if os.path.exists(data_file_path):
            try:
                df = pd.read_csv(data_file_path, encoding="utf-8-sig")
                logging.info(f"ğŸ“¥ æˆåŠŸåŠ è½½æ•°æ®ï¼š{data_file_path}ï¼ˆå…±{len(df)}æ¡ï¼‰")
            except Exception as e:
                logging.error(f"âŒ UTF-8ç¼–ç è¯»å–æ•°æ®å¤±è´¥ï¼ˆ{data_file_path}ï¼‰ï¼š{str(e)}")
                try:
                    df = pd.read_csv(data_file_path, encoding="latin1")
                    logging.info(f"ğŸ“¥ å¤‡ç”¨ç¼–ç ï¼ˆlatin1ï¼‰åŠ è½½æ•°æ®æˆåŠŸï¼š{data_file_path}")
                except Exception as e2:
                    logging.error(f"âŒ å¤‡ç”¨ç¼–ç è¯»å–æ•°æ®å¤±è´¥ï¼ˆ{data_file_path}ï¼‰ï¼š{str(e2)}")

        if os.path.exists(anno_file_path):
            try:
                with open(anno_file_path, "r", encoding="utf-8") as f:
                    annotations_data = json.load(f)
                for ann in annotations_data:
                    jama.append(set(ann.get("jama", [])))
                    gqs.append(int(ann.get("gqs", 1)))
                    discern.append(set(ann.get("discern", [])))
                    for col in meta.get("custom_columns", []):
                        col_name = col["name"]
                        col_type = col["type"]
                        if col_name not in custom_data:
                            custom_data[col_name] = []
                        value = ann.get(col_name, set() if col_type == "multi" else "")
                        if col_type == "multi" and isinstance(value, list):
                            value = set(value)
                        custom_data[col_name].append(value)
                logging.info(f"ğŸ“¥ æˆåŠŸåŠ è½½æ³¨è§£æ•°æ®ï¼š{anno_file_path}ï¼ˆå…±{len(annotations_data)}æ¡ï¼‰")
            except Exception as e:
                logging.error(f"âŒ åŠ è½½æ³¨è§£æ•°æ®å¤±è´¥ï¼ˆ{anno_file_path}ï¼‰ï¼š{str(e)}")
                jama = [set() for _ in range(len(df))]
                gqs = [1 for _ in range(len(df))]
                discern = [set() for _ in range(len(df))]
                for col in meta.get("custom_columns", []):
                    col_name = col["name"]
                    col_type = col["type"]
                    custom_data[col_name] = [set() if col_type == "multi" else "" for _ in range(len(df))]
        else:
            logging.warning(f"âš ï¸ æ³¨è§£æ–‡ä»¶ä¸å­˜åœ¨ï¼š{anno_file_path}ï¼Œåˆå§‹åŒ–ç©ºæ³¨è§£")
            jama = [set() for _ in range(len(df))]
            gqs = [1 for _ in range(len(df))]
            discern = [set() for _ in range(len(df))]
            for col in meta.get("custom_columns", []):
                col_name = col["name"]
                col_type = col["type"]
                custom_data[col_name] = [set() if col_type == "multi" else "" for _ in range(len(df))]

        if len(df) > len(jama):
            jama.extend([set() for _ in range(len(df) - len(jama))])
            gqs.extend([1 for _ in range(len(df) - len(gqs))])
            discern.extend([set() for _ in range(len(df) - len(discern))])
            for col in meta.get("custom_columns", []):
                col_name = col["name"]
                col_type = col["type"]
                custom_data[col_name].extend([set() if col_type == "multi" else "" for _ in range(len(df) - len(custom_data[col_name]))])
            logging.info(f"ğŸ“ æ‰©å±•æ³¨è§£é•¿åº¦ä»¥åŒ¹é…æ•°æ®ï¼šåŸæ³¨è§£{len(jama)-len(df)+len(jama)}æ¡ â†’ æ–°æ³¨è§£{len(jama)}æ¡")

        return df, jama, gqs, discern, custom_data

    def add_history(self, filename, data, custom_columns):
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        crawl_folder = os.path.join(self.history_dir, timestamp)
        os.makedirs(crawl_folder, exist_ok=True)
        logging.info(f"ğŸ“ åˆ›å»ºæ–°å†å²è®°å½•ç›®å½•ï¼š{crawl_folder}")

        meta = {
            "timestamp": timestamp,
            "filename": filename,
            "count": len(data),
            "custom_columns": custom_columns
        }
        meta_file_path = os.path.join(crawl_folder, "meta.json")
        data_file_path = os.path.join(crawl_folder, "data.csv")
        anno_file_path = os.path.join(crawl_folder, "annotations.json")

        try:
            with open(meta_file_path, "w", encoding="utf-8") as f:
                json.dump(meta, f, ensure_ascii=False, indent=2)
            logging.info(f"ğŸ“ æˆåŠŸä¿å­˜å…ƒæ•°æ®ï¼š{meta_file_path}")
        except Exception as e:
            logging.error(f"âŒ ä¿å­˜å…ƒæ•°æ®å¤±è´¥ï¼ˆ{meta_file_path}ï¼‰ï¼š{str(e)}")

        try:
            df = pd.DataFrame(data)
            expected_columns = [
                "title", "publish_time", "author_name", "like_count", "comment_count",
                "share_count", "collect_count", "video_url", "danmaku_count", "duration",
                "video_id", "play_count", "author_official_role", "is_verified"
            ]
            for col in expected_columns:
                if col not in df.columns:
                    if col in ["like_count", "comment_count", "share_count", "collect_count",
                               "danmaku_count", "play_count", "author_official_role"]:
                        df[col] = 0
                    else:
                        df[col] = ""
            df.to_csv(data_file_path, index=False, encoding="utf-8-sig")
            logging.info(f"ğŸ“ æˆåŠŸä¿å­˜æ•°æ®ï¼š{data_file_path}ï¼ˆå…±{len(df)}æ¡ï¼Œ{len(df.columns)}ä¸ªå­—æ®µï¼‰")
        except Exception as e:
            logging.error(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥ï¼ˆ{data_file_path}ï¼‰ï¼š{str(e)}")

        empty_annotations = [
            {
                "jama": [],
                "gqs": 1,
                "discern": [],
                **{col["name"]: [] if col["type"] == "multi" else "" for col in custom_columns}
            }
            for _ in range(len(data))
        ]
        try:
            with open(anno_file_path, "w", encoding="utf-8") as f:
                json.dump(empty_annotations, f, ensure_ascii=False, indent=2)
            logging.info(f"ğŸ“ æˆåŠŸåˆå§‹åŒ–ç©ºæ³¨è§£ï¼š{anno_file_path}ï¼ˆå…±{len(empty_annotations)}æ¡ï¼‰")
        except Exception as e:
            logging.error(f"âŒ åˆå§‹åŒ–æ³¨è§£å¤±è´¥ï¼ˆ{anno_file_path}ï¼‰ï¼š{str(e)}")

        return meta

    def save_annotations(self, meta, jama, gqs, discern, custom_data):
        crawl_folder = os.path.join(self.history_dir, meta["timestamp"])
        anno_file_path = os.path.join(crawl_folder, "annotations.json")

        annotations = [
            {
                "jama": list(jama[i]),
                "gqs": gqs[i],
                "discern": list(discern[i]),
                **{col_name: list(custom_data[col_name][i]) if isinstance(custom_data[col_name][i], set) else custom_data[col_name][i] for col_name in custom_data}
            }
            for i in range(len(jama))
        ]

        try:
            with open(anno_file_path, "w", encoding="utf-8") as f:
                json.dump(annotations, f, ensure_ascii=False, indent=2)
            logging.info(f"ğŸ“ æˆåŠŸä¿å­˜æ³¨è§£ï¼š{anno_file_path}ï¼ˆå…±{len(annotations)}æ¡ï¼‰")
        except Exception as e:
            logging.error(f"âŒ ä¿å­˜æ³¨è§£å¤±è´¥ï¼ˆ{anno_file_path}ï¼‰ï¼š{str(e)}")

    def save_custom_columns(self, meta, custom_columns):
        if not meta:
            return
        crawl_folder = os.path.join(self.history_dir, meta["timestamp"])
        meta_file_path = os.path.join(crawl_folder, "meta.json")
        try:
            with open(meta_file_path, "r", encoding="utf-8") as f:
                meta_data = json.load(f)
            meta_data["custom_columns"] = custom_columns
            with open(meta_file_path, "w", encoding="utf-8") as f:
                json.dump(meta_data, f, ensure_ascii=False, indent=2)
            logging.info(f"ğŸ“ æˆåŠŸä¿å­˜è‡ªå®šä¹‰å­—æ®µï¼š{meta_file_path}")
        except Exception as e:
            logging.error(f"âŒ ä¿å­˜è‡ªå®šä¹‰å­—æ®µå¤±è´¥ï¼ˆ{meta_file_path}ï¼‰ï¼š{str(e)}")

    def delete_history(self, meta):
        crawl_folder = os.path.join(self.history_dir, meta["timestamp"])
        if not os.path.exists(crawl_folder) or not os.path.isdir(crawl_folder):
            logging.warning(f"âš ï¸ å¾…åˆ é™¤çš„å†å²è®°å½•ç›®å½•ä¸å­˜åœ¨ï¼š{crawl_folder}")
            return False
        try:
            import shutil
            shutil.rmtree(crawl_folder)
            logging.info(f"ğŸ—‘ï¸ æˆåŠŸåˆ é™¤å†å²è®°å½•ï¼š{crawl_folder}ï¼ˆæ–‡ä»¶åï¼š{meta['filename']}ï¼‰")
            return True
        except Exception as e:
            logging.error(f"âŒ åˆ é™¤å†å²è®°å½•å¤±è´¥ï¼ˆ{crawl_folder}ï¼‰ï¼š{str(e)}")
            return False